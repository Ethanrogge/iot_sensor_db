\documentclass[12pt]{article}   % Texte plus lisible

\usepackage[a4paper,margin=2cm]{geometry}  % Marges r√©duites
\usepackage{graphicx} % Required for inserting images      % For including images
\usepackage{caption}       % Better control over captions
\usepackage{subcaption}    % If you want subfigures (optional)
\usepackage{amsmath}       % Math formulas
\usepackage{amssymb}       % Math symbols
\usepackage{booktabs}      % For nicer tables (optional)
\usepackage{hyperref}      % For clickable references
\usepackage{float}         % For forcing figure placement (H)
\usepackage{xcolor}
% Colors if needed

\title{Benchmarking Embedded Databases for IoT Applications}
\author{Ethan Rogge, ...}
\date{December 2025}

\begin{document}
\maketitle
\section{Introduction to the Technology: Embedded Databases}
\label{sec:technology}

Embedded databases constitute a class of data management systems designed to
run \emph{inside} an application rather than as an external server process.
The DBMS executes within the same address space as the host program and is
invoked via direct API calls instead of network protocols. This architectural
choice profoundly shapes both the capabilities and limitations of embedded
database systems.

\subsection{Definition of Embedded Databases}

An embedded database is a DBMS that:
\begin{itemize}
    \item operates as a software library linked to an application,
    \item does not require a separate server or daemon,
    \item stores data locally on the device's filesystem or in memory,
    \item handles queries or operations through a direct programming interface.
\end{itemize}

This contrasts with traditional server-based DBMS---such as PostgreSQL or
MySQL---where applications connect via network protocols to an external 
process that manages concurrency, storage, and query execution.

Embedded databases offer several key advantages:
\begin{itemize}
    \item \textbf{Low latency:} no network round-trip is required.
    \item \textbf{Minimal overhead:} no inter-process communication.
    \item \textbf{Portability:} the database engine is distributed with the application.
    \item \textbf{Small footprint:} suitable for embedded devices, IoT nodes, or mobile apps.
    \item \textbf{Tight control:} the application can directly manage caching and storage.
\end{itemize}

However, these systems also come with inherent limitations:
\begin{itemize}
    \item reduced concurrency and multi-user capabilities,
    \item limited or absent SQL support in many engines,
    \item no access control or advanced security features,
    \item responsibility for durability and schema evolution often shifted to the application.
\end{itemize}

These trade-offs make embedded databases particularly suitable for
resource-constrained environments such as IoT nodes, sensors, robotics, or
standalone desktop analytics applications.

\subsection{Architectural Models of Embedded Databases}

Embedded database technologies vary significantly in their internal design.
Several architectural models are commonly used:

\subsubsection{Page-oriented storage engines}

These systems manage their own page cache, buffer pools, and on-disk B-tree
structures. Examples include BerkeleyDB, SQLite, and LMDB. Their properties:
\begin{itemize}
    \item efficient key-based lookups,
    \item transactional semantics,
    \item predictable read/write performance,
    \item mature caching and recovery mechanisms.
\end{itemize}

\subsubsection{Log-structured merge trees (LSM)}

Engines such as LevelDB or RocksDB use LSM trees to support:
\begin{itemize}
    \item extremely fast writes through append-only logs,
    \item background compaction processes,
    \item excellent performance for streaming or time-series ingestion.
\end{itemize}

These engines are widely used in mobile apps, IoT gateways, and distributed
storage systems requiring sustained write throughput.

\subsubsection{In-process analytical engines}

Modern embedded engines such as DuckDB embed a complete analytical SQL 
engine directly inside the application. Their distinguishing features include:
\begin{itemize}
    \item columnar storage,
    \item vectorised execution and query pipelining,
    \item efficient analytical queries (aggregations, joins, window functions),
    \item no need for a separate database server.
\end{itemize}

Such systems illustrate the evolution of embedded technology: beyond simple
key--value stores, they support full SQL analytics locally within the
application.

\subsection{Query Capabilities in Embedded Systems}

Because embedded engines target environments with constrained resources or
specific workloads, their query capabilities vary widely.

\paragraph{Record-oriented APIs.}
Many engines expose operations such as:
\[
\texttt{get(key)},\quad \texttt{put(key,value)},\quad \texttt{delete(key)}
\]
These APIs offer very fast local access and are ideal for caching or
metadata storage.

\paragraph{Full SQL support.}
Some embedded DBMS such as SQLite or DuckDB support full SQL semantics.
This enables local execution of analytical queries, joins, and aggregations
without a remote server. Such systems are attractive for standalone desktop
analytics and local data processing pipelines.

\paragraph{Hybrid architectures.}
Certain applications combine multiple embedded technologies:
\begin{itemize}
    \item a lightweight key--value store for fast caching or ingestion,
    \item a more expressive embedded engine for analytical queries.
\end{itemize}

This hybrid model is precisely the one adopted by our application.

\subsection{Embedded Databases in IoT and Edge Computing}

IoT devices frequently require real-time data processing close to where data
is generated. Embedded databases are central to these architectures because
they allow devices to:
\begin{itemize}
    \item store recent sensor readings locally,
    \item perform preliminary filtering or anomaly detection,
    \item operate autonomously during network outages,
    \item reduce bandwidth by transmitting only summarised data,
    \item respect privacy constraints by keeping raw data on device.
\end{itemize}

Edge computing architectures often include:
\begin{itemize}
    \item microcontrollers or single-board computers running a lightweight embedded store,
    \item intermediate nodes performing local analytics,
    \item cloud or server components performing long-term storage or heavy computations.
\end{itemize}

Thus, embedded databases form the backbone of many modern distributed sensing
and monitoring systems.

\subsection{Relevance to Our Application}

The dataset used in our project (Intel Lab sensor data) represents a typical
IoT scenario involving time-series measurements collected by multiple devices.
The application requires:
\begin{itemize}
    \item fast access to recent readings,
    \item local computation of sensor statistics,
    \item detection of anomalies or outages,
    \item global analytics over large volumes of historical data.
\end{itemize}

These requirements naturally motivate a hybrid architecture:
\begin{itemize}
    \item a lightweight embedded store for fast lookups and caching,
    \item an embedded analytical engine for SQL-based analysis,
    \item a traditional server-based DBMS for baseline comparison.
\end{itemize}

\section{Introduction to the Tools}
\label{sec:tools}

In this section, we examine the two database management systems chosen to
illustrate embedded database technology: \textbf{BerkeleyDB} and \textbf{DuckDB}.
Although both systems belong to the family of embedded DBMS, they represent
two fundamentally different models: BerkeleyDB is a classical key--value engine,
while DuckDB is a modern analytical SQL engine designed to run in-process.
For completeness and for comparison with a traditional relational DBMS, we also
consider \textbf{PostgreSQL} as a baseline server-based engine.

Each subsection explains the internal architecture of the tool, its execution
model, and its strengths and limitations with respect to the requirements of
our IoT application.

% ---------------------------------------------------------------------------

\subsection{BerkeleyDB}
\label{sec:berkeleydb}

BerkeleyDB (BDB) is a mature, high-performance embedded storage engine that
provides key--value access through a simple API. It operates as a library linked
directly into the application, without any external server process. As one of
the earliest and most widely deployed embedded databases, it has been used in
operating systems, networking equipment, web browsers, and mobile devices.

\subsubsection{Architecture}
\label{sec:berkeleydb-architecture}

BerkeleyDB is built around a \textbf{page-oriented storage engine} and uses
B-trees as its primary on-disk structure. Data pages are cached in an internal
buffer pool, and the engine manages its own journaling, logging, and recovery
mechanisms. It exposes a simple interface:
\[
\texttt{put(key, value)}, \quad \texttt{get(key)}, \quad \texttt{delete(key)},
\]
with optional transactional semantics.

Since the DBMS runs within the same process as the application, all operations
bypass the overhead of inter-process communication. This leads to extremely low
latency for key lookups and updates.

\subsubsection{Core Features}
\label{sec:berkeleydb-features}

The main features of BerkeleyDB include:
\begin{itemize}
    \item \textbf{High performance} for point reads, inserts and updates.
    \item \textbf{Configurable storage models}, including B-tree, hash tables, and queues.
    \item \textbf{Transaction support} with ACID guarantees.
    \item \textbf{Crash recovery} through write-ahead logging.
    \item \textbf{Small footprint}, making it suitable for embedded systems and IoT devices.
\end{itemize}

\subsubsection{Transaction Model and Concurrency}
\label{sec:berkeleydb-transactions}

BerkeleyDB supports multiversion concurrency control (MVCC) and fine-grained
locking. However, concurrency is generally limited to threads within the same
process. Unlike server-based DBMS, BerkeleyDB does not natively serve multiple
external clients simultaneously.

\subsubsection{Limitations}
\label{sec:berkeleydb-limitations}

Despite its performance advantages, BerkeleyDB lacks:
\begin{itemize}
    \item SQL query capabilities,
    \item built-in analytical operators (aggregations, joins, windows),
    \item multi-user concurrency architecture,
    \item ad-hoc querying capabilities.
\end{itemize}

Any form of analytics must be implemented manually within the application.
For this reason, BerkeleyDB is best viewed as a \textbf{local cache or storage
component} rather than a general-purpose DBMS.

\subsubsection{Relevance to Embedded Applications}
\label{sec:berkeleydb-relevance}

BerkeleyDB is extremely well suited for applications that require:
\begin{itemize}
    \item storing recent data locally,
    \item fast retrieval of time-critical information,
    \item operation in resource-constrained devices,
    \item offline operation or intermittent connectivity.
\end{itemize}

In our IoT scenario, these characteristics make BerkeleyDB the ideal engine for
storing and retrieving the most recent sensor readings at very low latency.

% ---------------------------------------------------------------------------

\subsection{DuckDB}
\label{sec:duckdb}

DuckDB is a modern embedded database designed specifically for analytical
workloads. It provides a full SQL interface and implements many of the
optimisations found in columnar analytical systems such as MonetDB or
Analytical Processing Systems, but packaged as a lightweight in-process
library.

\subsubsection{Architecture}
\label{sec:duckdb-architecture}

DuckDB follows a \textbf{columnar storage model} and uses vectorised execution.
Instead of processing one row at a time, DuckDB processes data in fixed-size
vectors (e.g., 1024 rows), greatly improving CPU efficiency and cache locality.
It employs late materialisation, compressed storage, and pipelined execution
to optimise analytical queries.

The engine runs entirely within the host process, eliminating the need for a
database server and reducing overhead for local analytical tasks.

\subsubsection{Vectorized Execution Model}
\label{sec:duckdb-vectorized}

Vectorised execution allows DuckDB to:
\begin{itemize}
    \item evaluate expressions over batches of rows,
    \item exploit SIMD instructions,
    \item reduce interpretation overhead,
    \item pipeline operations efficiently.
\end{itemize}

This model is particularly advantageous for scans, aggregations, joins, and
window functions, making DuckDB well suited for analytical workloads.

\subsubsection{Columnar Storage and Query Processing}
\label{sec:duckdb-columnar}

DuckDB stores tables in columnar format, meaning that values of the same column
are stored contiguously. This enables:
\begin{itemize}
    \item faster sequential scans for analytical queries,
    \item data compression,
    \item better I/O efficiency,
    \item reduced memory footprint.
\end{itemize}

Unlike server DBMS such as PostgreSQL, DuckDB does not maintain row-oriented
storage optimised for transactional workloads. This explains its poor
performance on UPDATE, DELETE, and frequent point-lookups.

\subsubsection{Strengths and Limitations}
\label{sec:duckdb-limitations}

\paragraph{Strengths.}
\begin{itemize}
    \item State-of-the-art analytical performance.
    \item Fully embedded, no server required.
    \item Full SQL support including window functions.
    \item Highly portable and easy to integrate into applications.
\end{itemize}

\paragraph{Limitations.}
\begin{itemize}
    \item Very slow for write-heavy workloads (UPDATE, DELETE).
    \item Poor performance for random point-lookups.
    \item Not a multi-user or concurrent transactional system.
\end{itemize}

\subsubsection{Role in Analytical Embedded Workloads}
\label{sec:duckdb-relevance}

DuckDB is ideal for applications requiring:
\begin{itemize}
    \item local execution of analytical SQL queries,
    \item processing of large datasets in batch,
    \item interactive analytics without deploying a database server,
    \item embedded ETL or data science pipelines.
\end{itemize}

In our application, DuckDB is responsible for executing analytics such as
anomaly detection, aggregations, top-k selection, and temporal analyses.


\section{Common Assessment}
\label{sec:assessment}

In this section, we evaluate how the three database systems --- BerkeleyDB,
DuckDB, and PostgreSQL --- behave when applied to the requirements of our IoT
monitoring application. We first describe the dataset and the application
workload, then present our benchmarking methodology. The benchmark results
themselves are provided in Section~\ref{sec:results}.

\subsection{Application Description}
\label{sec:application}

The objective of the application is to process real-world IoT sensor data
collected from a wireless sensor network. The application must support both
\textbf{local, low-latency operations} (e.g., retrieving the most recent reading
of a sensor) and \textbf{global analytical tasks} (e.g., computing averages,
detecting anomalies, finding outages). This dual requirement motivates the use
of embedded databases and provides a meaningful context in which to compare the
three database engines.

\subsubsection{Dataset Description}
\label{sec:dataset}

We use the publicly available Intel Berkeley Research Lab dataset, a widely
studied real-world IoT dataset. It contains:
\begin{itemize}
    \item over 2.3 million timestamped readings,
    \item collected from 54 sensors deployed indoors,
    \item covering temperature, humidity, light intensity, and voltage,
    \item timestamped both as \texttt{epoch} and as human-readable date/time.
\end{itemize}

Each row corresponds to the measurement of a single sensor at a specific time.
This dataset is particularly suited to evaluating embedded databases because:
\begin{itemize}
    \item it resembles typical IoT monitoring workloads,
    \item it is large enough to stress test analytical engines,
    \item it requires both fast point lookups and global computations.
\end{itemize}

For the benchmark, the raw \texttt{data.txt} file is converted to CSV and
loaded into the different database engines. To simulate deployments of various
scales, subsamples of size $1\,000$, $10\,000$, and $100\,000$ rows are created
using a stratified sampling procedure that preserves the chronological order
within each sensor.

\subsubsection{Application Requirements}
\label{sec:application-requirements}

The IoT application processes both \emph{recent} and \emph{historical} data.
Its main requirements include:
\begin{itemize}
    \item \textbf{rapid access to the most recent sensor values}, possibly in
          real-time,
    \item \textbf{local statistical computations} on one or more sensors,
    \item \textbf{global analytical queries} over large subsets of historical data,
    \item \textbf{detection of anomalies} in temperature patterns,
    \item \textbf{detection of outages}, i.e., time gaps in the readings,
    \item \textbf{comparisons between sensors}, such as correlation analysis.
\end{itemize}

These requirements naturally lead to distinct workload patterns, some of which
are latency-sensitive (e.g., LAST\_READINGS) while others are throughput- or
scan-intensive (e.g., OUTAGES or DAILY\_SUMMARY).

\subsubsection{Implemented Functionalities}
\label{sec:functionalities}

The application provides eleven functionalities, each corresponding to one or
more query types evaluated in the benchmark. The most relevant operations are:
\begin{itemize}
    \item \textbf{Ingestion (LOAD)}: insert a batch of readings.
    \item \textbf{Point lookup (LAST\_READINGS, READ)}: retrieve recent sensor values.
    \item \textbf{Updates \& deletes}: modify or remove existing readings.
    \item \textbf{Aggregations}: average temperature per sensor (AVG\_SENSOR),
          daily summaries (DAILY\_SUMMARY), or top-K sensors (TOPK\_SENSORS).
    \item \textbf{Anomaly detection}: identify values that deviate from the mean.
    \item \textbf{Outage detection}: find time gaps using window functions.
    \item \textbf{Sensor comparison}: compute cross-sensor correlations.
    \item \textbf{Full profile}: a combined workload summarising multiple operations.
\end{itemize}

Each functionality maps cleanly to one or more benchmark phases. This ensures
that the benchmark reflects realistic requirements rather than synthetic tasks.

\subsubsection{Hybrid Architecture}
\label{sec:hybrid-architecture}

The application adopts a hybrid architecture reflecting the strengths and
limitations of each database system:
\begin{itemize}
    \item \textbf{BerkeleyDB} is used as a \emph{local embedded cache} storing
          the most recent sensor readings. Its extremely fast key--value access
          makes it ideal for point lookups and small-scale local statistics.
    \item \textbf{DuckDB} is used as an \emph{embedded analytical engine}
          providing full SQL capabilities. It executes most analytical queries,
          such as aggregations, window functions, and correlation analysis.
    \item \textbf{PostgreSQL} serves as a \emph{baseline reference} for
          evaluating the strengths and weaknesses of embedded DBMS compared to a
          traditional server-based relational engine.
\end{itemize}

This architecture mirrors typical IoT and edge-computing deployments, where:
\begin{itemize}
    \item the \textbf{device} stores recent readings locally,
    \item an \textbf{embedded engine} performs analytics on device or gateway,
    \item a \textbf{server} stores or aggregates data at scale.
\end{itemize}

The hybrid model also highlights how embedded databases complement rather than
replace traditional relational systems.

% ---------------------------------------------------------------------------

\subsection{Benchmark Methodology}
\label{sec:methodology}

To evaluate the performance of the three systems, we designed a benchmark based
directly on the application's requirements. The benchmark includes ingestion,
point lookups, updates, deletions, aggregations, anomaly detection, and temporal
analysis operations. Our objective is to study:
\begin{itemize}
    \item how each system scales with increasing dataset size,
    \item whether the behaviour is linear or exponential,
    \item how embedded systems compare to a traditional RDBMS.
\end{itemize}

\subsubsection{Workload Definition}
\label{sec:workload}

Each benchmark phase corresponds to a specific application-level task:
\begin{itemize}
    \item \textbf{LOAD}: insert $N$ rows.
    \item \textbf{READ} and \textbf{LAST\_READINGS}: retrieve recent sensor values.
    \item \textbf{UPDATE} / \textbf{DELETE}: modify or remove records.
    \item \textbf{AVG\_SENSOR}, \textbf{TOPK\_SENSORS}: compute aggregated statistics.
    \item \textbf{ANOMALIES}: detect outliers relative to average temperature.
    \item \textbf{OUTAGES}: detect time gaps using window functions.
    \item \textbf{COMPARE\_TWO}: sensor correlation analysis.
    \item \textbf{FULL\_PROFILE}: combined analytical workload.
\end{itemize}

These phases collectively model typical IoT monitoring tasks involving both
transactional and analytical operations.

\subsubsection{Data Scales}
\label{sec:data-scales}

To assess scalability, we evaluate the benchmark using three dataset sizes:
\[
N \in \{ 1\,000,\; 10\,000,\; 100\,000 \}.
\]

For each size, a stratified sampling algorithm selects an equal number of rows
per sensor whenever possible and preserves chronological order. This ensures that
the benchmark is representative of the full dataset and preserves time-series
structure.

\subsubsection{Execution Protocol}
\label{sec:execution-protocol}

Each benchmark phase is executed six times consecutively. The first execution is
discarded because it typically includes caching, compilation, or buffer pool
initialisation effects. The average of the remaining five executions is used as
the reported performance metric.

This procedure follows best practices in benchmarking, including:
\begin{itemize}
    \item warming up caches to avoid cold-start bias,
    \item averaging over multiple runs to reduce variance,
    \item ensuring reproducibility across systems.
\end{itemize}
\section{Benchmark Results}
\label{sec:benchmark-results}

This section presents the performance obtained for the full set of benchmark phases
executed on the three database engines: \emph{BerkeleyDB}, \emph{DuckDB}, and
\emph{PostgreSQL}. All experiments were executed for
$N \in \{10^3, 10^4, 10^5\}$ rows. Each phase was run six times; the first
execution was discarded to avoid cache warm-up effects, and the execution times
reported in the figures correspond to the average of the remaining five runs.

We distinguish between (i) OLTP-like operations (LOAD, READ, UPDATE, DELETE,
LAST\_READINGS) and (ii) analytical SQL operations (AVG\_SENSOR, HOT\_SENSORS,
ANOMALIES, DAILY\_SUMMARY, OUTAGES, TOPK\_SENSORS, COMPARE\_TWO, FULL\_PROFILE).

\subsection{OLTP-like operations}

\paragraph{LOAD.}
Figure~\ref{fig:load} shows the time to load $N$ rows into each DBMS.
DuckDB exhibits very poor scalability, increasing from a few seconds at $10^3$
rows to more than 330\,s for $10^5$ rows. This linear but steep growth is due
to its immutable storage model and internal checkpointing during bulk inserts.
PostgreSQL remains efficient, staying below 1.5\,s even for $10^5$ rows thanks
to optimized WAL handling and bulk insert mechanisms.
BerkeleyDB is by far the fastest (below 0.1\,s for all $N$), illustrating the
efficiency of a key--value store for append-heavy workloads.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\linewidth]{fig/load.png}
  \caption{LOAD --- average duration vs.~number of rows.}
  \label{fig:load}
\end{figure}

\paragraph{READ.}
The READ benchmark retrieves records by key (sensor identifier). As shown in
Figure~\ref{fig:read}, BerkeleyDB dominates with sub-millisecond response
times. DuckDB and PostgreSQL both scale linearly, but DuckDB becomes extremely
slow at $10^5$ rows (more than 70\,s), confirming that it is not optimized for
repeated point lookups. PostgreSQL remains significantly faster (below 26\,s),
relying on index-based lookups.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\linewidth]{fig/read_duration.png}
  \caption{READ --- average duration vs.~number of rows.}
  \label{fig:read}
\end{figure}

\paragraph{UPDATE and DELETE.}
Figures~\ref{fig:update} and~\ref{fig:delete} show that BerkeleyDB again
provides the best performance with almost constant-time updates and deletions.
PostgreSQL is slower but scales predictably, reaching around 17\,s for UPDATE
and 3\,s for DELETE at $10^5$ rows. DuckDB is by far the slowest (over
240\,s for UPDATE and 11\,s for DELETE at $10^5$), due to table rewrites
induced by its append-only columnar storage.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\linewidth]{fig/update_duration.png}
  \caption{UPDATE --- average duration vs.~number of rows.}
  \label{fig:update}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\linewidth]{fig/delete_duration.png}
  \caption{DELETE --- average duration vs.~number of rows.}
  \label{fig:delete}
\end{figure}

\paragraph{LAST\_READINGS.}
This phase simulates an IoT device requesting the most recent readings for a
set of sensors. Figure~\ref{fig:lastreadings} shows that BerkeleyDB is the
fastest (fractions of milliseconds even at $10^5$ rows). DuckDB performs
moderately well ($\approx 21$\,ms at $10^5$), while PostgreSQL is the slowest
($\approx 31$\,ms at $10^5$), though still within acceptable limits.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\linewidth]{fig/last_readings.png}
  \caption{LAST\_READINGS --- average duration vs.~number of rows.}
  \label{fig:lastreadings}
\end{figure}

\subsection{Analytical SQL operations}

\paragraph{ANOMALIES.}
The ANOMALIES phase computes deviations from the mean temperature for each
sensor using SQL aggregations and window logic. As shown in
Figure~\ref{fig:anomalies}, DuckDB is consistently faster (from 0.006\,s to
0.022\,s) than PostgreSQL (from 0.002\,s to 0.070\,s), thanks to vectorized
columnar execution. BerkeleyDB cannot execute this query.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\linewidth]{fig/anomalies.png}
  \caption{ANOMALIES --- performance comparison between DuckDB and PostgreSQL.}
  \label{fig:anomalies}
\end{figure}

\paragraph{AVG\_SENSOR.}
Figure~\ref{fig:avgsensor} presents the average execution time to compute the
average temperature per sensor. DuckDB clearly outperforms PostgreSQL,
achieving about 0.01\,s at $10^5$ rows, versus 0.027\,s for PostgreSQL.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\linewidth]{fig/avg_sensor.png}
  \caption{AVG\_SENSOR --- performance comparison.}
  \label{fig:avgsensor}
\end{figure}

\paragraph{HOT\_SENSORS.}
The HOT\_SENSORS query returns sensors whose average temperature exceeds a
threshold and have enough readings. As seen in Figure~\ref{fig:hotsensors},
both systems scale linearly, but DuckDB is systematically faster than
PostgreSQL.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\linewidth]{fig/hot_sensors.png}
  \caption{HOT\_SENSORS --- performance comparison.}
  \label{fig:hotsensors}
\end{figure}

\paragraph{OUTAGES.}
The OUTAGES phase detects large gaps in the time series using a LAG window
function. Figure~\ref{fig:outages} shows that DuckDB scales from roughly
0.003\,s to 0.014\,s, while PostgreSQL increases from 0.001\,s to about
0.040\,s. Both are linear, but DuckDB has a smaller slope.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\linewidth]{fig/outages.png}
  \caption{OUTAGES --- performance comparison.}
  \label{fig:outages}
\end{figure}

\paragraph{TOPK\_SENSORS.}
For the TOPK\_SENSORS query (top sensors by average temperature),
Figure~\ref{fig:topk} confirms DuckDB's advantage: its execution time remains
around three times lower than PostgreSQL, while both curves grow linearly with
$N$.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\linewidth]{fig/topk_sensors.png}
  \caption{TOPK\_SENSORS --- performance comparison.}
  \label{fig:topk}
\end{figure}

\paragraph{COMPARE\_TWO.}
The COMPARE\_TWO query computes the correlation between the temperatures of two
sensors. Figure~\ref{fig:comparetwo} shows that DuckDB remains fast
($\approx 0.005$\,s at $10^5$), whereas PostgreSQL is considerably slower
($\approx 0.029$\,s).

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\linewidth]{fig/compare_two.png}
  \caption{COMPARE\_TWO --- performance comparison.}
  \label{fig:comparetwo}
\end{figure}

\paragraph{DAILY\_SUMMARY.}
Interestingly, Figure~\ref{fig:dailysummary} shows that PostgreSQL is faster
than DuckDB for the DAILY\_SUMMARY aggregation. DuckDB reaches about 0.050\,s
at $10^5$ rows, while PostgreSQL stays near 0.023\,s. This can be explained by
differences in how both engines materialise intermediate grouped results.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\linewidth]{fig/daily_summary.png}
  \caption{DAILY\_SUMMARY --- performance comparison.}
  \label{fig:dailysummary}
\end{figure}

\paragraph{FULL\_PROFILE.}
Finally, Figure~\ref{fig:fullprofile} presents the FULL\_PROFILE workload.
For DuckDB and PostgreSQL, this corresponds to the combined cost of several
analytical queries (AVG\_SENSOR, DAILY\_SUMMARY, OUTAGES, TOPK\_SENSORS),
whereas BerkeleyDB executes a local statistical summary on its cached data.
DuckDB provides the best analytical performance (0.012\,s to 0.078\,s),
PostgreSQL is slower (0.018\,s to 0.105\,s), and BerkeleyDB is extremely fast
for its limited local task (0.004\,s to 0.030\,s).

\begin{figure}[H]
  \centering
  \includegraphics[width=0.4\linewidth]{fig/full_profile.png}
  \caption{FULL\_PROFILE --- combined profile workload.}
  \label{fig:fullprofile}
\end{figure}


\section{Discussion}
\label{sec:discussion}

The experimental results highlight a clear separation of responsibilities
between the three engines tested. Each system exhibits strengths that are
consistent with its architectural design.

\subsection{BerkeleyDB}

\paragraph{Strengths.}
BerkeleyDB offers outstanding performance for key--value access patterns. It
provides near-constant-time READ, UPDATE, and DELETE operations, and extremely
fast LOAD times even for $10^5$ rows. Its footprint is small and it can be
embedded directly in applications, making it an excellent candidate for
resource-constrained IoT devices.

\paragraph{Weaknesses.}
BerkeleyDB does not support SQL and cannot execute analytical queries such as
aggregations, joins, or window functions. All higher-level logic must be
implemented in the application code. As a result, BerkeleyDB is unsuitable as a
standalone analytical database and must be complemented by another engine for
global analysis.

\subsection{DuckDB}

\paragraph{Strengths.}
DuckDB delivers the best performance for all analytical queries in our
benchmark (ANOMALIES, AVG\_SENSOR, HOT\_SENSORS, OUTAGES, TOPK\_SENSORS,
COMPARE\_TWO, and the analytical part of FULL\_PROFILE). Its columnar storage
and vectorised execution make it particularly well adapted to OLAP workloads on
large sensor datasets, while still running as an embedded library without a
separate server process.

\paragraph{Weaknesses.}
On the other hand, DuckDB performs extremely poorly on OLTP-like workloads:
LOAD, UPDATE, DELETE and repeated point READs are orders of magnitude slower
than with BerkeleyDB or even PostgreSQL. This is a direct consequence of its
append-only storage and optimisation for scans rather than random writes. It is
therefore not suitable for write-heavy or latency-sensitive transactional
workloads.

\subsection{PostgreSQL}

\paragraph{Strengths.}
PostgreSQL provides balanced behaviour across workloads. It is much faster than
DuckDB for UPDATE and DELETE operations, and its index-based execution plans
offer reasonable performance for READ and LAST\_READINGS. For analytics, it is
systematically slower than DuckDB, but still scales linearly and remains within
low tens of milliseconds for $10^5$ rows. As a mature relational system, it
also offers full transactional guarantees, rich indexing options and a robust
client--server architecture.

\paragraph{Weaknesses.}
PostgreSQL is not optimised for columnar scans and cannot match DuckDB on pure
analytical workloads. It also has a higher operational overhead (separate
server process, configuration, memory usage) than embedded engines such as
DuckDB or BerkeleyDB.

\subsection{Overall assessment}

Taken together, the results show that no single engine dominates all workloads.
Instead, each engine is optimal for a different layer of the IoT architecture:

\begin{itemize}
  \item \textbf{Device / edge layer:} BerkeleyDB is ideal as a local cache of
    recent readings, thanks to its extremely fast key--value access and tiny
    footprint.
  \item \textbf{Local analytics layer:} DuckDB is the best choice for complex
    analytical queries and batch processing over large sensor datasets.
  \item \textbf{Server / baseline layer:} PostgreSQL provides a robust,
    general-purpose relational backend and serves as a solid reference for
    comparison.
\end{itemize}

This validates the architectural choice made in our application: use
BerkeleyDB to store the most recent sensor readings for low-latency access, and
delegate heavier analytical workloads to DuckDB, while PostgreSQL acts as a
traditional relational baseline. The benchmark confirms that embedded
key--value technology is extremely well suited for the local IoT cache layer,
but must be complemented by an analytical engine to satisfy the full set of
application requirements.


\end{document}
